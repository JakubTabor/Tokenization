{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. John is swimming in a boat.\n",
      "bird steel the mans sandwich\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Mr. John is swimming in a boat. bird steel the mans sandwich\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "    \n",
    "# It split (tokenize) my sentences smartly, because not every dot is end of the phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.\n",
      "John\n",
      "is\n",
      "swimming\n",
      "in\n",
      "a\n",
      "boat\n",
      ".\n",
      "bird\n",
      "steel\n",
      "the\n",
      "mans\n",
      "sandwich\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)\n",
    "# It print individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. John is swimming in a boat.', 'bird steel the mans sandwich']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(\"Mr. John is swimming in a boat. bird steel the mans sandwich\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'John',\n",
       " 'is',\n",
       " 'swimming',\n",
       " 'in',\n",
       " 'a',\n",
       " 'boat',\n",
       " '.',\n",
       " 'bird',\n",
       " 'steel',\n",
       " 'the',\n",
       " 'mans',\n",
       " 'sandwich']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "word_tokenize(\"Mr. John is swimming in a boat. bird steel the mans sandwich\")\n",
    "\n",
    "# Space give me best algorithm, in nltk i must try my options but You have many algorithms and field for customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.\n",
      "John\n",
      "is\n",
      "a\n",
      "timberman\n",
      "he\n",
      "loves\n",
      "doing\n",
      "his\n",
      "job\n",
      "and\n",
      "he\n",
      "eat\n",
      "sandwiches\n",
      "for\n",
      "2\n",
      "$\n",
      "during\n",
      "his\n",
      "job\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Mr. John is a timberman he loves doing his job and he eat sandwiches for 2$ during his job\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let`s\n",
      "go\n",
      "to\n",
      "Tokio\n",
      ".\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp('''\"Let`s go to Tokio.!\"''')\n",
    "\n",
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[1 : 4]\n",
    "type(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_1 = doc[1]\n",
    "token_1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_15 = doc[15]\n",
    "token_15.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_16 = doc[16]\n",
    "token_16.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_15.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_1.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_16.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_15.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. ==> index:  0 is_alpha: False is_punct: False like_num: False is_currency: False\n",
      "John ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "is ==> index:  2 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "a ==> index:  3 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "timberman ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "he ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "loves ==> index:  6 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "doing ==> index:  7 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "his ==> index:  8 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "job ==> index:  9 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "and ==> index:  10 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "he ==> index:  11 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "eat ==> index:  12 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "sandwiches ==> index:  13 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "for ==> index:  14 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "2 ==> index:  15 is_alpha: False is_punct: False like_num: True is_currency: False\n",
      "$ ==> index:  16 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "during ==> index:  17 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "his ==> index:  18 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "job ==> index:  19 is_alpha: True is_punct: False like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i,\n",
    "          \"is_alpha:\", token.is_alpha,\n",
    "          \"is_punct:\", token.is_punct,\n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Mikel   5 June, 1882    mikel@green.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@rock.com\\n',\n",
       " 'Serena  24 June, 1998   serena@neils.com \\n',\n",
       " 'Joe      1 May, 1997    joe@big.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Mikel   5 June, 1882    mikel@green.com\\n Maria\\t12 April, 2001  maria@rock.com\\n Serena  24 June, 1998   serena@neils.com \\n Joe      1 May, 1997    joe@big.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mikel@green.com', 'maria@rock.com', 'serena@neils.com', 'joe@big.com']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "emails = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR  |  PRON  |  True  |  False  |  False  |  pronoun\n",
      ".  |  PUNCT  |  False  |  False  |  False  |  punctuation\n",
      "José  |  PROPN  |  True  |  False  |  False  |  proper noun\n",
      "tiene  |  VERB  |  True  |  False  |  False  |  verb\n",
      "un  |  DET  |  True  |  False  |  False  |  determiner\n",
      "sombrero  |  NOUN  |  True  |  False  |  False  |  noun\n",
      "por  |  ADP  |  True  |  False  |  False  |  adposition\n",
      "4  |  NUM  |  False  |  False  |  True  |  numeral\n",
      "€  |  NOUN  |  False  |  True  |  False  |  noun\n",
      ",  |  PUNCT  |  False  |  False  |  False  |  punctuation\n",
      "el  |  DET  |  True  |  False  |  False  |  determiner\n",
      "sombrero  |  NOUN  |  True  |  False  |  False  |  noun\n",
      "protege  |  NOUN  |  True  |  False  |  False  |  noun\n",
      "para  |  ADP  |  True  |  False  |  False  |  adposition\n",
      "el  |  DET  |  True  |  False  |  False  |  determiner\n",
      "sol  |  NOUN  |  True  |  False  |  False  |  noun\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "doc = nlp(\"SR. José tiene un sombrero por 4€, el sombrero protege para el sol\") # MR. José has a hat for €4, the hat protects against the sun\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.is_alpha, \" | \", token.is_currency, \" | \", token.like_num, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "氏  |  NOUN  |  True  |  False  |  False  |  noun\n",
      "。  |  PUNCT  |  False  |  False  |  False  |  punctuation\n",
      "ヒロシ  |  PROPN  |  True  |  False  |  False  |  proper noun\n",
      "は  |  ADP  |  True  |  False  |  False  |  adposition\n",
      "700  |  NUM  |  False  |  False  |  True  |  numeral\n",
      "¥  |  SYM  |  False  |  True  |  False  |  symbol\n",
      "で  |  ADP  |  True  |  False  |  False  |  adposition\n",
      "お  |  NOUN  |  True  |  False  |  False  |  noun\n",
      "寿司  |  NOUN  |  True  |  False  |  False  |  noun\n",
      "を  |  ADP  |  True  |  False  |  False  |  adposition\n",
      "食べ  |  VERB  |  True  |  False  |  False  |  verb\n",
      "ます  |  AUX  |  True  |  False  |  False  |  auxiliary\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"ja\")\n",
    "\n",
    "doc = nlp(\"氏。ヒロシは700¥でお寿司を食べます\") #Mr. Hiroshi eats sushi for 700 yen.\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.is_alpha, \" | \", token.is_currency, \" | \", token.like_num, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'that', 'key']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"gimme that key\")\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'that', 'key']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH : \"gim\"},\n",
    "    {ORTH : \"me\"}\n",
    "])\n",
    "\n",
    "doc = nlp(\"gimme that key\")\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x25bedaf5510>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer') # i add some components after tokenizer to my pipeline | now my nlp object know how to split paragraph sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. John is swimming in a boat.\n",
      "bird steel the mans sandwich\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mr. John is swimming in a boat. bird steel the mans sandwich\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The code and data used in this book are available from http://\n",
    "allendowney.github.io/ThinkStats2/.\n",
    "The easiest way to work with this code it to run it on Colab, which is a free\n",
    "service that runs Jupyter notebooks in a web browser. For every chapter,\n",
    "I provide two notebooks: one contains the code from the chapter and the\n",
    "exercises; the other also contains the solutions.\n",
    "If you want to run these notebooks on your own computer, you\n",
    "can downloads them individually from GitHub or download the en\u0002tire repository from https://github.com/AllenDowney/ThinkStats2/\n",
    "archive/refs/heads/master.zip.\n",
    "I developed this book using Anaconda from Continuum Analytics, which is a\n",
    "free Python distribution that includes all the packages you’ll need to run the\n",
    "code (and lots more). I found Anaconda easy to install. By default it does a\n",
    "user-level installation, so you don’t need administrative privileges. You can\n",
    "download Anaconda from http://continuum.io/downloads.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://',\n",
       " 'https://github.com/AllenDowney/ThinkStats2/',\n",
       " 'master.zip',\n",
       " 'http://continuum.io/downloads']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "data_websites = [token.text for token in doc if token.like_url]\n",
    "data_websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "transactions = \"John need to retire three $ to Emma and Emma need to pay 500€ for her bills\"\n",
    "\n",
    "doc = nlp(transactions)\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i + 1].is_currency:\n",
    "        print(token.text, doc[token.i + 1].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
